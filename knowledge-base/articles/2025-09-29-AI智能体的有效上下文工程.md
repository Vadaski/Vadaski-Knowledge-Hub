# AI智能体的有效上下文工程

**来源**: [Anthropic Engineering](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)  
**发布日期**: 2025-09-29  
**摘要**: 上下文是AI智能体的关键但有限的资源。在这篇文章中，我们探讨了有效策划和管理驱动智能体的上下文的策略。  
---

在提示工程（prompt engineering）成为应用AI领域关注焦点几年后，一个新的术语开始崭露头角：**上下文工程**（context engineering）。使用语言模型构建应用正变得越来越少关注为提示找到正确的词语和短语，而更多关注回答一个更广泛的问题："什么样的上下文配置最有可能产生我们模型期望的行为？"

**上下文**指的是从大语言模型（LLM）采样时包含的token集合。当前的**工程**问题是在LLM的固有约束下优化这些token的效用，以持续实现期望的结果。有效驾驭LLM通常需要**上下文思维**——换句话说：考虑LLM在任何给定时间可用的整体状态，以及该状态可能产生的潜在行为。

在这篇文章中，我们将探讨新兴的上下文工程艺术，并为构建可引导、有效的智能体提供一个精炼的心智模型。


## 上下文工程 vs. 提示工程

在Anthropic，我们将上下文工程视为提示工程的自然演进。提示工程指的是为获得最佳结果而编写和组织LLM指令的方法（参见我们的文档了解概述和有用的提示工程策略）。**上下文工程**指的是在LLM推理过程中策划和维护最优token（信息）集合的策略，包括可能出现在提示之外的所有其他信息。

在LLM工程的早期，提示是AI工程工作的最大组成部分，因为除了日常聊天交互之外的大多数用例都需要针对一次性分类或文本生成任务优化的提示。正如这个术语所暗示的，提示工程的主要焦点是如何编写有效的提示，特别是系统提示。然而，随着我们向构建更强大的智能体迈进，这些智能体在多次推理轮次和更长的时间范围内运行，我们需要管理整个上下文状态的策略（系统指令、工具、模型上下文协议（MCP）、外部数据、消息历史等）。

在循环中运行的智能体会生成越来越多可能与下一轮推理相关的数据，这些信息必须循环优化。上下文工程是从不断演化的可能信息宇宙中策划将进入有限上下文窗口的内容的艺术和科学。


## 为什么上下文工程对构建强大的智能体很重要

尽管LLM具有速度和处理越来越大数据量的能力，但我们观察到，LLM像人类一样，在某个时刻会失去焦点或感到困惑。关于"大海捞针"式基准测试的研究揭示了**上下文衰减**（context rot）的概念：随着上下文窗口中token数量的增加，模型从该上下文中准确回忆信息的能力会下降。

虽然某些模型表现出比其他模型更温和的退化，但这种特性在所有模型中都会出现。因此，上下文必须被视为具有递减边际收益的有限资源。就像人类具有有限的工作记忆容量一样，LLM在解析大量上下文时有一个"注意力预算"。每个新引入的token都会以某种程度消耗这个预算，增加了仔细策划LLM可用token的需求。

这种注意力稀缺源于LLM的架构约束。LLM基于**transformer架构**，这使得每个token都能关注整个上下文中的每个其他token。这导致n个token产生n²个成对关系。

随着上下文长度的增加，模型捕获这些成对关系的能力变得薄弱，在上下文大小和注意力焦点之间产生自然张力。此外，模型从训练数据分布中发展其注意力模式，其中较短的序列通常比较长的序列更常见。这意味着模型对上下文范围依赖关系的经验较少，专门的参数也较少。

像位置编码插值这样的技术允许模型通过将它们适应到最初训练的较小上下文来处理更长的序列，尽管在token位置理解方面会有一些退化。这些因素创造了性能梯度而不是硬性悬崖：模型在较长上下文中仍然非常强大，但与在较短上下文中的性能相比，可能在信息检索和长程推理方面表现出降低的精度。

这些现实意味着，深思熟虑的上下文工程对于构建强大的智能体至关重要。


## 有效上下文的构成

鉴于LLM受到有限注意力预算的约束，良好的上下文工程意味着找到最小可能的高信号token集合，以最大化某些期望结果的可能性。实施这种做法说起来容易做起来难，但在以下部分中，我们概述了这一指导原则在不同上下文组件中的实际含义。

系统提示应该极其清晰，使用简单、直接的语言，以适合智能体的正确高度呈现想法。正确的高度是两种常见失败模式之间的"金发姑娘区域"（Goldilocks zone）。在一个极端，我们看到工程师在提示中硬编码复杂、脆弱的逻辑来引发精确的智能体行为。这种方法会随着时间的推移产生脆弱性并增加维护复杂性。在另一个极端，工程师有时提供模糊的、高层次的指导，无法为LLM提供期望输出的具体信号，或错误地假设共享上下文。最优高度取得平衡：足够具体以有效引导行为，又足够灵活以为模型提供强大的启发式来引导行为。

我们建议将提示组织成不同的部分（如`<background_information>`、`<instructions>`、`## Tool guidance`、`## Output description`等），并使用XML标记或Markdown标题等技术来划分这些部分，尽管随着模型变得更强大，提示的确切格式可能变得不那么重要。

无论你如何决定构建系统提示，你都应该努力实现完全概述你期望行为的最小信息集合。（注意，最小不一定意味着短；你仍然需要提前给智能体足够的信息，以确保它遵循期望的行为。）最好先用可用的最佳模型测试一个最小提示，看看它在你的任务上的表现如何，然后根据初始测试中发现的失败模式添加清晰的指令和示例来改进性能。

工具允许智能体与其环境交互，并在工作时拉入新的、额外的上下文。因为工具定义了智能体与其信息/行动空间之间的契约，所以工具促进效率是极其重要的，既要通过返回token高效的信息，也要通过鼓励高效的智能体行为。

在《为AI智能体编写工具——使用AI智能体》中，我们讨论了构建LLM易于理解且功能重叠最小的工具。类似于设计良好的代码库的函数，工具应该是自包含的、对错误具有鲁棒性的，并且就其预期用途而言极其清晰。输入参数应该同样具有描述性、明确，并发挥模型的固有优势。

我们看到的最常见的失败模式之一是臃肿的工具集，它们覆盖了太多功能，或导致关于使用哪个工具的模糊决策点。如果人类工程师无法明确说出在给定情况下应该使用哪个工具，就不能期望AI智能体做得更好。正如我们稍后将讨论的，为智能体策划一个最小可行的工具集也可以导致在长时间交互中更可靠的维护和上下文修剪。

提供示例，也称为少样本提示（few-shot prompting），是我们继续强烈建议的众所周知的最佳实践。然而，团队经常将一长串边缘情况塞进提示中，试图阐明LLM应该为特定任务遵循的每个可能规则。我们不推荐这样做。相反，我们建议努力策划一组多样化的、规范的示例，有效描绘智能体的期望行为。对于LLM来说，示例是"一图胜千言"的"图片"。

我们在上下文不同组件（系统提示、工具、示例、消息历史等）上的总体指导是深思熟虑，保持你的上下文信息丰富但紧凑。现在让我们深入了解在运行时动态检索上下文。


## 上下文检索和智能体搜索

在《构建有效的AI智能体》中，我们强调了基于LLM的工作流和智能体之间的差异。自我们写那篇文章以来，我们已经倾向于对智能体的简单定义：在循环中自主使用工具的LLM。

与客户合作时，我们看到该领域正在向这个简单范式收敛。随着底层模型变得更强大，智能体的自主水平可以扩展：更智能的模型允许智能体独立导航细微的问题空间并从错误中恢复。

我们现在看到工程师在设计智能体上下文的方式上发生了转变。今天，许多AI原生应用采用某种形式的基于嵌入的推理前检索，以浮现重要上下文供智能体推理。随着该领域转向更智能体的方法，我们越来越多地看到团队用"及时"（just in time）上下文策略增强这些检索系统。

与其预先处理所有相关数据，使用"及时"方法构建的智能体维护轻量级标识符（文件路径、存储的查询、网络链接等），并使用这些引用在运行时使用工具动态将数据加载到上下文中。Anthropic的智能体编码解决方案Claude Code使用这种方法对大型数据库执行复杂的数据分析。模型可以编写有针对性的查询，存储结果，并利用像`head`和`tail`这样的Bash命令来分析大量数据，而无需将完整的数据对象加载到上下文中。这种方法反映了人类认知：我们通常不会记忆整个信息语料库，而是引入外部组织和索引系统，如文件系统、收件箱和书签，以按需检索相关信息。

除了存储效率，这些引用的元数据提供了一种有效优化行为的机制，无论是明确提供的还是直观的。对于在文件系统中运行的智能体，名为`test_utils.py`的文件存在于`tests`文件夹中，与位于`src/core_logic/`中的同名文件相比，意味着不同的目的。文件夹层次结构、命名约定和时间戳都提供了重要信号，帮助人类和智能体理解如何以及何时利用信息。

让智能体自主导航和检索数据还支持渐进式披露——换句话说，允许智能体通过探索逐步发现相关上下文。每次交互都会产生为下一个决策提供信息的上下文：文件大小暗示复杂性；命名约定暗示目的；时间戳可以是相关性的代理。智能体可以逐层组装理解，仅在工作记忆中维护必要的内容，并利用笔记策略进行额外的持久化。这种自我管理的上下文窗口使智能体专注于相关子集，而不是淹没在详尽但可能无关的信息中。

当然，有一个权衡：运行时探索比检索预计算数据慢。不仅如此，还需要有主见和深思熟虑的工程，以确保LLM拥有正确的工具和启发式来有效导航其信息景观。没有适当的指导，智能体可能会通过误用工具、追逐死胡同或未能识别关键信息来浪费上下文。

在某些设置中，最有效的智能体可能采用混合策略，预先检索一些数据以提高速度，并自行决定进行进一步的自主探索。"正确"自主水平的决策边界取决于任务。Claude Code是一个采用这种混合模型的智能体：`CLAUDE.md`文件被简单地预先放入上下文中，而像`glob`和`grep`这样的原语允许它导航其环境并及时检索文件，有效绕过过时索引和复杂语法树的问题。

混合策略可能更适合内容动态性较低的上下文，如法律或金融工作。随着模型能力的提高，智能体设计将趋向于让智能模型智能地行动，逐步减少人工策划。鉴于该领域的快速发展，"做最简单有效的事情"可能仍然是我们为在Claude之上构建智能体的团队提供的最佳建议。


### 长时程任务的上下文工程

长时程任务要求智能体在token数量超过LLM上下文窗口的动作序列中保持连贯性、上下文和目标导向的行为。对于跨越数十分钟到数小时连续工作的任务，如大型代码库迁移或综合研究项目，智能体需要专门的技术来绕过上下文窗口大小限制。

等待更大的上下文窗口可能看起来是一个明显的策略。但很可能在可预见的未来，所有大小的上下文窗口都将受到上下文污染和信息相关性问题的困扰——至少对于需要最强智能体性能的情况。为了使智能体在扩展的时间范围内有效工作，我们开发了一些直接解决这些上下文污染约束的技术：压缩（compaction）、结构化笔记（structured note-taking）和多智能体架构。

压缩是获取接近上下文窗口限制的对话，总结其内容，并用摘要重新启动新上下文窗口的做法。压缩通常作为上下文工程中的第一个杠杆，以推动更好的长期连贯性。压缩的核心是以高保真度提炼上下文窗口的内容，使智能体能够以最小的性能退化继续工作。

例如，在Claude Code中，我们通过将消息历史传递给模型来总结和压缩最关键细节来实现这一点。模型保留架构决策、未解决的错误和实施细节，同时丢弃冗余的工具输出或消息。然后智能体可以继续使用这个压缩的上下文加上最近访问的五个文件。用户获得连续性，而无需担心上下文窗口限制。

压缩的艺术在于选择保留什么与丢弃什么，因为过于激进的压缩可能导致微妙但关键的上下文丢失，其重要性只有在后来才变得明显。对于实施压缩系统的工程师，我们建议在复杂的智能体跟踪上仔细调整你的提示。首先最大化召回率，确保你的压缩提示捕获跟踪中的每条相关信息，然后迭代以提高精度，消除多余内容。

低悬的多余内容的一个例子是清除工具调用和结果——一旦工具在消息历史深处被调用，为什么智能体需要再次看到原始结果？最安全、最轻触的压缩形式之一是工具结果清除，最近作为Claude Developer Platform上的功能发布。

**结构化笔记**

结构化笔记，或智能体记忆，是一种技术，智能体定期将笔记持久化到上下文窗口之外的内存中。这些笔记在稍后的时间被拉回上下文窗口。

这种策略以最小的开销提供持久记忆。就像Claude Code创建待办事项列表，或你的自定义智能体维护`NOTES.md`文件一样，这种简单模式允许智能体跟踪复杂任务的进度，维护关键上下文和依赖关系，否则这些信息会在数十个工具调用中丢失。

Claude玩Pokémon展示了记忆如何在非编码领域转变智能体能力。智能体在数千个游戏步骤中保持精确计数——跟踪目标，如"在过去的1,234步中，我一直在Route 1训练我的Pokémon，Pikachu已经获得了8级，目标是10级。"在没有任何关于记忆结构的提示的情况下，它开发了已探索区域的地图，记住它解锁了哪些关键成就，并维护战斗策略的战略笔记，帮助它了解哪些攻击对不同对手最有效。

在上下文重置后，智能体读取自己的笔记并继续多小时的训练序列或地牢探索。这种跨摘要步骤的连贯性实现了长时程策略，如果仅将所有信息保留在LLM的上下文窗口中，这些策略将是不可能的。

作为我们Sonnet 4.5发布的一部分，我们在Claude Developer Platform上发布了内存工具的公开测试版，通过基于文件的系统更容易在上下文窗口之外存储和咨询信息。这允许智能体随着时间的推移构建知识库，跨会话维护项目状态，并引用以前的工作，而无需将所有内容保留在上下文中。

**子智能体架构**

子智能体架构提供了绕过上下文限制的另一种方式。与其让一个智能体尝试在整个项目中维护状态，专门的子智能体可以用干净的上下文窗口处理聚焦的任务。主智能体协调高级计划，而子智能体执行深度技术工作或使用工具查找相关信息。每个子智能体可能广泛探索，使用数万个token或更多，但只返回其工作的浓缩、提炼摘要（通常1,000-2,000个token）。

这种方法实现了清晰的关注点分离——详细的搜索上下文仍然隔离在子智能体内，而主导智能体专注于综合和分析结果。这种模式在《我们如何构建多智能体研究系统》中讨论，在复杂研究任务上显示出比单智能体系统的显著改进。

这些方法之间的选择取决于任务特征。例如：

即使模型继续改进，在扩展交互中保持连贯性的挑战仍将是构建更有效智能体的核心。

上下文工程代表了我们使用LLM构建方式的根本转变。随着模型变得更强大，挑战不仅仅是制作完美的提示——而是深思熟虑地策划在每个步骤中进入模型有限注意力预算的信息。无论你是为长时程任务实施压缩、设计token高效的工具，还是使智能体能够及时探索其环境，指导原则保持不变：找到最小的高信号token集合，以最大化你期望结果的可能性。

我们概述的技术将随着模型的改进而继续发展。我们已经看到，更智能的模型需要更少的规范性工程，允许智能体以更大的自主性运行。但即使能力扩展，将上下文视为珍贵的、有限的资源仍将是构建可靠、有效智能体的核心。

今天就在Claude Developer Platform上开始使用上下文工程，并通过我们的内存和上下文管理食谱访问有用的提示和最佳实践。


## 致谢

由Anthropic的应用AI团队撰写：Prithvi Rajasekaran、Ethan Dixon、Carly Ryan和Jeremy Hadfield，团队成员Rafi Ayub、Hannah Moran、Cal Rueb和Connor Jennings的贡献。特别感谢Molly Vorwerck、Stuart Ritchie和Maggie Vo的支持。
